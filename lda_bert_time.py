# -*- coding: utf-8 -*-
import os
import re
import warnings
import multiprocessing
from pathlib import Path
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

import pandas as pd
from tqdm import tqdm
import jieba
import torch

from gensim import corpora, models
from gensim.models import CoherenceModel
from sklearn.feature_extraction.text import CountVectorizer
from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
import pyLDAvis.gensim_models as gensimvis
import pyLDAvis


# ----------------------- å…¨å±€åˆ†è¯å‡½æ•° ----------------------- #
STOP_PATH = Path(__file__).parent / "stopword.txt"
stopwords = (
    {
        w.strip()
        for w in open(STOP_PATH, encoding="utf-8")
        if w.strip()
    }
    if STOP_PATH.exists()
    else set()
)


def clean_cut(txt: str):
    """å¾®åšæ–‡æœ¬æ¸…æ´— + åˆ†è¯"""
    txt = re.sub(r"#.*?#|@\w+|http\S+|Oç½‘é¡µé“¾æ¥", " ", str(txt))
    txt = re.sub(r"[^\w\s\u4e00-\u9fff]", " ", txt)
    return [w for w in jieba.lcut(txt) if len(w) > 1 and w not in stopwords]


# ----------------------------- ä¸»æµç¨‹ ------------------------------------ #
def main():
    warnings.filterwarnings("ignore")
    tqdm.pandas()

    # ---------- è·¯å¾„ ----------
    SCRIPT_DIR = Path(__file__).parent
    OUTPUTS_DIR = SCRIPT_DIR / "docs" / "outputs"
    MODELS_DIR = SCRIPT_DIR / "models"
    DATA_FILE = SCRIPT_DIR / "combined_annotated.csv"

    OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)
    MODELS_DIR.mkdir(parents=True, exist_ok=True)

    # ---------- è¯»å–æ•°æ® ----------
    assert DATA_FILE.exists(), f"æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {DATA_FILE}"
    df = pd.read_csv(DATA_FILE, dtype=str)
    assert {"å¾®åšæ­£æ–‡", "å‘å¸ƒæ—¶é—´"} <= set(df.columns), "CSV ç¼ºå°‘å¿…è¦åˆ—"

    df["å‘å¸ƒæ—¶é—´"] = pd.to_datetime(df["å‘å¸ƒæ—¶é—´"])

    # ---------- ä»…ä¿ç•™ 2024 å¹´æ•°æ® ----------
    mask_2024 = (df["å‘å¸ƒæ—¶é—´"] >= "2024-01-01") & (df["å‘å¸ƒæ—¶é—´"] <= "2024-12-31")
    df_2024 = df.loc[mask_2024]

    if df_2024.empty:
        print("âš ï¸ æ•°æ®ä¸­ä¸åŒ…å« 2024 å¹´å†…å®¹ï¼Œæµç¨‹ç»ˆæ­¢ã€‚")
        return

    segments = [("year_2024", df_2024)]

    # ---------- åµŒå…¥æ¨¡å‹ ----------
    embedder = SentenceTransformer(
        "paraphrase-multilingual-MiniLM-L12-v2",
        device="cuda" if torch.cuda.is_available() else "cpu",
    )

    # ---------- å¾ªç¯å¤„ç†ï¼ˆè¿™é‡Œåªæœ‰ä¸€ä¸ªåˆ†æ®µï¼‰ ----------
    for name, sub in segments:
        texts = sub["å¾®åšæ­£æ–‡"].fillna("").tolist()
        N = len(texts)
        print(f"\n=== å¤„ç†åˆ†æ®µ: {name}ï¼Œå…± {N} æ¡å¾®åš ===")

        # åŠ¨æ€è°ƒæ•´ vectorizer å‚æ•°
        if N < 50:
            vec = CountVectorizer(
                tokenizer=clean_cut,
                ngram_range=(1, 2),
                min_df=1,
                max_df=1.0,
            )
        else:
            vec = CountVectorizer(
                tokenizer=clean_cut,
                ngram_range=(1, 2),
                min_df=2,
                max_df=0.95,
            )

        token_lists = [clean_cut(t) for t in texts]

        # ----- 1) BERTopic -----
        topic_model = BERTopic(
            language="chinese",
            embedding_model=embedder,
            vectorizer_model=vec,
            min_topic_size=10,
            top_n_words=12,
            calculate_probabilities=True,
            verbose=False,
        )
        topic_model.fit(texts)

        # å®‰å…¨å°è¯•è‡ªåŠ¨åˆå¹¶ä¸»é¢˜
        try:
            topic_model = topic_model.reduce_topics(texts, nr_topics="auto")
        except Exception as e:
            print(f"âš ï¸ {name} auto-reduce_topics è·³è¿‡: {e}")

        seg_out = OUTPUTS_DIR / name
        seg_model = MODELS_DIR / name
        seg_out.mkdir(exist_ok=True)
        seg_model.mkdir(exist_ok=True)

        topic_model.save(str(seg_model / "bertopic_model"))
        topic_model.get_topic_info().to_csv(
            seg_out / "bertopic_topic_info.csv", index=False
        )
        topic_model.visualize_topics(width=1200, height=700).write_html(
            str(seg_out / "bertopic_topics.html")
        )

        # ----- 2) LDA (k=18) -----
        dictionary = corpora.Dictionary(token_lists)
        dictionary.filter_extremes(no_below=5, no_above=0.5)
        corpus = [dictionary.doc2bow(t) for t in token_lists]

        lda18 = models.LdaModel(
            corpus=corpus,
            id2word=dictionary,
            num_topics=18,
            passes=10,
            random_state=42,
        )
        lda18.save(str(seg_model / "lda_18.bin"))

        with open(seg_out / "lda_top_words_18.txt", "w", encoding="utf-8") as f:
            for i, topic in lda18.show_topics(18, 15, formatted=False):
                f.write(f"Topic {i:02d}: {' '.join(w for w, _ in topic)}\n")

        vis18 = gensimvis.prepare(lda18, corpus, dictionary, sort_topics=False)
        pyLDAvis.save_html(vis18, str(seg_out / "lda_vis_18.html"))

        # ----- 3) LDA k è°ƒä¼˜ -----
        print(" - è°ƒå‚ k ...")
        scores, best_k, best_c, best_model = [], -1, -1, None
        for k in range(8, 31, 2):
            m = models.LdaModel(
                corpus=corpus,
                id2word=dictionary,
                num_topics=k,
                passes=10,
                random_state=42,
            )
            c = (
                CoherenceModel(
                    model=m,
                    texts=token_lists,
                    dictionary=dictionary,
                    coherence="c_v",
                ).get_coherence()
            )
            scores.append((k, c))
            if c > best_c:
                best_k, best_c, best_model = k, c, m

        ks, cvs = zip(*scores)
        plt.figure(figsize=(8, 4.5))
        plt.plot(ks, cvs, marker="o")
        plt.xlabel("Num Topics (k)")
        plt.ylabel("Coherence (c_v)")
        plt.title("LDA Coherence vs k")
        plt.grid(alpha=0.3)
        plt.xticks(ks)
        plt.tight_layout()
        plt.savefig(seg_out / "lda_k_selection.png", dpi=150)
        plt.close()

        best_model.save(str(seg_model / f"lda_best_k{best_k}.bin"))
        print(f"âœ… [{name}] æœ€ä½³ k={best_k}, coherence={best_c:.4f}")

    print("\nğŸ‰ 2024 å¹´æ•°æ®å¤„ç†å®Œæ¯•ï¼Œç»“æœå·²å†™å…¥ docs/outputs/")


# ----------------------------- å…¥å£ ------------------------------------- #
if __name__ == "__main__":
    multiprocessing.freeze_support()
    main()
